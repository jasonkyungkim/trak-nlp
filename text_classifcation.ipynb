{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"kCUAlLf0G0ND"},"outputs":[],"source":["#download dataset here: https://www.kaggle.com/datasets/saurabhshahane/ecommerce-text-classification/data\n","\n","# i worked with this notebook in collab, so mounts will be paths on my drive\n","\n"]},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2X2ZCsjKgqrQ","executionInfo":{"status":"ok","timestamp":1701633552308,"user_tz":300,"elapsed":5412,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"6ee60726-68dc-4466-a4ab-392600d17bbc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"]}]},{"cell_type":"code","source":["pip install transformers datasets evaluate\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"33X_R715gquu","executionInfo":{"status":"ok","timestamp":1701633559809,"user_tz":300,"elapsed":7505,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"a3d7c732-c6fd-4f3a-e6d8-8d73547e5a25"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Collecting datasets\n","  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting evaluate\n","  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting pyarrow-hotfix (from datasets)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n","Collecting responses<0.19 (from evaluate)\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: pyarrow-hotfix, dill, responses, multiprocess, datasets, evaluate\n","Successfully installed datasets-2.15.0 dill-0.3.7 evaluate-0.4.1 multiprocess-0.70.15 pyarrow-hotfix-0.6 responses-0.18.0\n"]}]},{"cell_type":"code","source":["pip install --upgrade torch\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p4ZOs17jYPsG","executionInfo":{"status":"ok","timestamp":1701633694739,"user_tz":300,"elapsed":134941,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"622cebb7-3f67-481e-f215-130bd0a71cac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n","Collecting torch\n","  Downloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch)\n","  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n","  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.1.0+cu118\n","    Uninstalling torch-2.1.0+cu118:\n","      Successfully uninstalled torch-2.1.0+cu118\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n","torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n","torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\n","torchvision 0.16.0+cu118 requires torch==2.1.0, but you have torch 2.1.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 torch-2.1.1\n"]}]},{"cell_type":"code","source":["pip install accelerate -U"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M0s3k6rDqaYK","executionInfo":{"status":"ok","timestamp":1701633700677,"user_tz":300,"elapsed":5956,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"c86869e1-ab6b-4b32-8236-a133e84c29c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting accelerate\n","  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.1)\n","Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.19.4)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n","Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.18.1)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.25.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r4aN-JZSJwv1"},"outputs":[],"source":["from datasets import load_dataset\n","\n","# this line loads squad from huggingface, but we arent working with that dataset\n","#squad = load_dataset(\"squad\", split=\"train[:5000]\")\n"]},{"cell_type":"code","source":["import pandas as pd\n","from datasets import Dataset\n","#load csv downloaded from kaggle\n","# Load your CSV file into a Pandas DataFrame\n","\"\"\"csv_path = '/content/drive/MyDrive/ecommerceDataset.csv'\n","df = pd.read_csv(csv_path,names=['class','text'])#\n","\n","\n","\n","\"\"\""],"metadata":{"id":"VIQrcx26UoOF","executionInfo":{"status":"ok","timestamp":1701633705019,"user_tz":300,"elapsed":125,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"265c614b-69b6-4c1b-89b6-255aef007f2d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"csv_path = '/content/drive/MyDrive/ecommerceDataset.csv'\\ndf = pd.read_csv(csv_path,names=['class','text'])#\\n\\n\\n\\n\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["#df.class.unique()"],"metadata":{"id":"4xa87UnhgfDo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#df['class'].unique()"],"metadata":{"id":"iSiWw6hheuXH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# drop null rows\n","#df = df.dropna()\n","#df[df['text'].isnull()==True]"],"metadata":{"id":"cYeqRYMYkFor"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#catgorical to nummerical id\n","#df['class_int'] = pd.factorize(df['class'])[0].astype(int)\n"],"metadata":{"id":"oHD8qwLVn2SQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#df.head()"],"metadata":{"id":"zQzZPsvdn8KY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#df[['class','class_int']].drop_duplicates()"],"metadata":{"id":"QkQF5JUwmuVN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"dataset = Dataset.from_pandas(df)\n","\n","\n","# Print the first few examples to verify the dataset\n","print(dataset)\n","## Save the dataset if needed\n","#hf_dataset.save_to_disk('your_dataset_name')\n","dataset['text'][0]\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"p8aiwFDletUX","executionInfo":{"status":"ok","timestamp":1701633792538,"user_tz":300,"elapsed":101,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"8d0c842a-0024-416f-a5ce-7e6e36a4b1e2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"dataset = Dataset.from_pandas(df)\\n\\n\\n# Print the first few examples to verify the dataset\\nprint(dataset)\\n## Save the dataset if needed\\n#hf_dataset.save_to_disk('your_dataset_name')\\ndataset['text'][0]\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":[],"metadata":{"id":"FE0DBfhtiSmd"}},{"cell_type":"code","source":[],"metadata":{"id":"utCAHYGyYNxU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import csv\n","import itertools as it\n","import numpy as np\n","import sklearn.decomposition\n","np.random.seed(0)\n","from tqdm import tqdm\n","\n"],"metadata":{"id":"ZNm7OO6xKPLR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from datasets import Dataset\n","\n","# Load your CSV file into a Pandas DataFrame\n","csv_path = '/content/drive/MyDrive/ecommerceDataset.csv'\n","df = pd.read_csv(csv_path,names=['class','text'])#\n","df=df.dropna()\n","df=df.sample(frac=1) #shuffle so not in class order\n","df['class_int'] = pd.factorize(df['class'])[0].astype(int)\n","\n","df_train = df.sample(frac=.20)\n","df_eval = df.drop(df_train.index)\n","\n","#500 clothing samples\n","df_eval = df_eval[df_eval['class_int'] == 3].iloc[:500]\n","\n"],"metadata":{"id":"j9ly57sHKPN6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%pdb\n","#df_holdout = df[df['class_int'] == 2]\n","#df = df.drop(df[df['class_int'] == 2].index)\n","#remap class int label so model\n","#df.loc[df.class_int >= 2,'class_int'] =df.loc[df.class_int >= 2,'class_int'] -1\n","print(df.class_int.unique())\n","\n","\n","holdout_train_dfs = []\n","holdout_eval_dfs = []\n","holdsout_train = {}\n","for samp in [0.0,.01,.1,1]:\n","\n","\n","  df_train_ho_class = df_train[df_train['class_int'] == 3]\n","  df_train_ho_drop = df_train_ho_class.sample(frac=(1-samp))\n","  df_train_ho = df_train.drop(df_train_ho_drop.index)\n","\n","  holdsout_train[samp] = df_train_ho\n","\n","\n","holdsout_train['eval'] = df_eval\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JIWJHhPTmGID","executionInfo":{"status":"ok","timestamp":1701635660882,"user_tz":300,"elapsed":145,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"37bd30a2-4c5b-4cd2-a170-20d350430c8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Automatic pdb calling has been turned OFF\n","[0 1 2 3]\n"]}]},{"cell_type":"code","source":["holdsout_train.keys()"],"metadata":{"id":"S2KdSDg-KPQU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701635661781,"user_tz":300,"elapsed":2,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"641f4be4-7341-489e-8930-fe3e52abdde3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_keys([0.0, 0.01, 0.1, 1, 'eval'])"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["ls drive/MyDrive/mit-nlp-project"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dkNi2VuUWPTX","executionInfo":{"status":"ok","timestamp":1701635662881,"user_tz":300,"elapsed":165,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"2ecce91d-436e-42ec-8132-ee1d63aabc55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;36mdrive/MyDrive/mit-nlp-project\u001b[0m@\n"]}]},{"cell_type":"code","source":["import pickle\n","from google.colab import drive\n","drive.mount('/content/drive')\n","file_name = f'drive/MyDrive/mit-nlp-project/final_datasets_0_.01_.1_1.pickle'\n","with open(file_name, 'wb') as handle:\n","    pickle.dump(holdsout_train, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lcARe4bqWAHS","executionInfo":{"status":"ok","timestamp":1701635669943,"user_tz":300,"elapsed":1107,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"b1287910-3306-45b5-8cc5-09d50ebf9c4e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import transformers\n","transformers\n","print(transformers.__version__)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6fdPMecFPO8T","executionInfo":{"status":"ok","timestamp":1701635671386,"user_tz":300,"elapsed":136,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"bad10e22-34fd-4a2a-98c5-1c423ad2ade5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4.35.2\n"]}]},{"cell_type":"code","source":["# from transformers import AutoTokenizer\n","from transformers import DistilBertTokenizer\n","tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')"],"metadata":{"id":"jxWKtoG2KPS5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import TensorDataset\n","def tokenizer_function(input_data, labels):\n","  input_ids = []\n","  attention_masks = []\n","  for sent in input_data:\n","    this_encoding = tokenizer.encode_plus(sent, truncation=True, pad_to_max_length = True,max_length = 512,return_attention_mask = True,return_tensors = 'pt')\n","    input_ids.append(this_encoding['input_ids'])\n","    attention_masks.append( this_encoding['attention_mask'])\n","    #print(this_encoding['input_ids'])\n","    #print(this_encoding['attention_mask'])\n","  input_ids = torch.cat(input_ids, dim=0)\n","  attention_masks = torch.cat(attention_masks, dim=0)\n","  labels = torch.tensor(labels)\n","  tokenized_data = TensorDataset(input_ids, attention_masks, labels)\n","  return tokenized_data"],"metadata":{"id":"RXBqruyAKPVB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#small datset for smoke test\n","\n","\n","#train_reviews, train_labels = list(df.text[:train_end]), list(df['class_int'][:train_end])\n","#val_reviews, val_labels = list(df.text[train_end:val_end]), list(df['class_int'][train_end:val_end])\n","val_reviews, val_labels = list(holdsout_train['eval'].text), list(holdsout_train['eval']['class_int'])\n","\n","datasets = {}\n","for k,v in holdsout_train.items():\n","  if k != 'eval':\n","    train_reviews, train_labels = list(v.text), list(v['class_int'])\n","\n","\n","    tr = train_reviews#[::] + ho_train_reviews\n","    tl = train_labels#[::] + ho_train_labels\n","    vr = val_reviews#[::] + ho_val_reviews\n","    vl = val_labels#[::]  + ho_val_labels\n","    #indices_eval = np.arange(len(vr)).tolist()\n","    #indices_train = np.arange(len(tr)).tolist()\n","\n","    train_dataset = tokenizer_function(tr,tl)#([tr[i] for i in indices_train],[tl[i] for i in indices_train] )\n","    val_dataset = tokenizer_function(vr,vl)#[vr[i] for i in indices_eval],[vl[i] for i in indices_eval] )\n","\n","    datasets[k] = [train_dataset,val_dataset]\n","\n","\n","\n","#train_dataset = tokenizer_function(train_reviews, train_labels )\n","#val_dataset = tokenizer_function(val_reviews, val_labels )\n","datasets\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U6W52OddMGP_","executionInfo":{"status":"ok","timestamp":1701635757156,"user_tz":300,"elapsed":82980,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"e3f0873d-e250-408f-e448-4654fd770283"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["{0.0: [<torch.utils.data.dataset.TensorDataset at 0x7b0f931a1540>,\n","  <torch.utils.data.dataset.TensorDataset at 0x7b0ed4cb7220>],\n"," 0.01: [<torch.utils.data.dataset.TensorDataset at 0x7b0ed532e350>,\n","  <torch.utils.data.dataset.TensorDataset at 0x7b10070d53c0>],\n"," 0.1: [<torch.utils.data.dataset.TensorDataset at 0x7b0ed432b610>,\n","  <torch.utils.data.dataset.TensorDataset at 0x7b0ed4328e20>],\n"," 1: [<torch.utils.data.dataset.TensorDataset at 0x7b0ed432a140>,\n","  <torch.utils.data.dataset.TensorDataset at 0x7b0ed432b0a0>]}"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["import pickle\n","from google.colab import drive\n","drive.mount('/content/drive')\n","file_name = f'drive/MyDrive/final_datasets_0_.01_.1_1.pickle'\n","with open(file_name, 'wb') as handle:\n","    pickle.dump(datasets, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HMMgRjvtvVRK","executionInfo":{"status":"ok","timestamp":1701635767997,"user_tz":300,"elapsed":1650,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"4ea190d3-42cc-4ad9-cdc5-3e4203998596"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"bgFFUdhRvFns"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","from google.colab import drive\n","drive.mount('/content/drive')\n","file_name = f'drive/MyDrive/mit-nlp-project/final_datasets_0_.01_.1_1.pickle'\n","with open(file_name, 'rb') as handle:\n","    ds = pickle.load(handle)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rElD6p3tbjjA","executionInfo":{"status":"ok","timestamp":1702139860002,"user_tz":300,"elapsed":2113,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"3996c417-4565-4858-8f32-b50364a7b721"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["for k in ds.keys():\n","  print('========')\n","  print(k)\n","  print(ds[k]['class'].value_counts())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9giFqjSncPGU","executionInfo":{"status":"ok","timestamp":1702140131928,"user_tz":300,"elapsed":349,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"7cde7c3f-a3ba-4dab-c25c-64d24fea5cce"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["========\n","0.0\n","Household                 3789\n","Electronics               2160\n","Clothing & Accessories    1683\n","Name: class, dtype: int64\n","========\n","0.01\n","Household                 3789\n","Electronics               2160\n","Clothing & Accessories    1683\n","Books                       25\n","Name: class, dtype: int64\n","========\n","0.1\n","Household                 3789\n","Electronics               2160\n","Clothing & Accessories    1683\n","Books                      245\n","Name: class, dtype: int64\n","========\n","1\n","Household                 3789\n","Books                     2453\n","Electronics               2160\n","Clothing & Accessories    1683\n","Name: class, dtype: int64\n","========\n","eval\n","Books    500\n","Name: class, dtype: int64\n"]}]},{"cell_type":"code","source":["|from transformers import DistilBertForSequenceClassification, AdamW\n","from torch.utils.data import DataLoader\n","import torch.nn.functional as F\n"],"metadata":{"id":"adyYwuzfLwib"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"metadata":{"id":"jtwxHbZ5L2Ja"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","\n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"metadata":{"id":"gUWEoocZM063"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-ILYfPeAPwJ3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["n_lab = len(df['class'].unique())\n","print(df['class'].unique())\n","print(f\"Number of classes: {n_lab}\")\n","print(f\"Number of training examples: {len(train_dataset)}\")\n","print(f\"Number of validation examples: {len(val_dataset)} \")"],"metadata":{"id":"VUia4dQiRMGJ","executionInfo":{"status":"ok","timestamp":1701635836772,"user_tz":300,"elapsed":367,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"efb8791c-4a9b-453b-e45d-a93255aa7f94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Electronics' 'Household' 'Clothing & Accessories' 'Books']\n","Number of classes: 4\n","Number of training examples: 10085\n","Number of validation examples: 500 \n"]}]},{"cell_type":"code","source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","device"],"metadata":{"id":"HQ8iY3uNkJ0R","executionInfo":{"status":"ok","timestamp":1701635848374,"user_tz":300,"elapsed":413,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b8545736-1e61-4ca2-e873-0797bddc17e0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["import gc\n","#torch.cuda.empty_cache()\n","for k,v in datasets.items():\n","  train_dataset,val_dataset = v\n","  print(f'undersampled percentage {k}')\n","  gc.collect()\n","  #for lr in [5e-3,5e-5,5e-7]:\n","  lr = 5e-05\n","  for bs in [32]:\n","    gc.collect()\n","    #torch.cuda.empty_cache()\n","    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=n_lab, output_attentions = False, output_hidden_states=False)\n","    model.to(device)\n","\n","\n","    optimizer = AdamW(model.parameters(),\n","                    lr = lr, # args.learning_rate - default is 5e-5.\n","                    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                  )\n","    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True) #Feel free to experiment with batch sizes\n","    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n","\n","\n","    total_t0 = time.time()\n","\n","    for epoch in tqdm(range(3)):\n","\n","      print(f'\\n Epoch {epoch}')\n","      total_train_loss = 0\n","      model.train()\n","      total_train_accuracy=0\n","      for step, batch in enumerate(train_loader):\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        model.zero_grad()\n","        outputs = model(b_input_ids,\n","                                attention_mask=b_input_mask,\n","                                labels=b_labels)\n","        loss = outputs.loss\n","        total_train_loss += loss\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","        if(step %10 == 0):\n","          print(loss)\n","\n","        logits = outputs.logits\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        total_train_accuracy += flat_accuracy(logits, label_ids)\n","\n","      avg_train_loss = total_train_loss / len(train_loader)\n","      print(\"\")\n","      print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","\n","      print(\"\")\n","      print(\"Running Validation...\")\n","\n","      t0 = time.time()\n","\n","      # Put the model in evaluation mode-\n","      model.eval()\n","\n","      # Tracking variables\n","      total_eval_accuracy = 0\n","      total_eval_loss = 0\n","      nb_eval_steps = 0\n","\n","        # Evaluate data for one epoch\n","      for batch in val_loader:\n","          #\n","          # `batch` contains three pytorch tensors:\n","          #   [0]: input ids\n","          #   [1]: attention masks\n","          #   [2]: labels\n","          b_input_ids = batch[0].to(device)\n","          b_input_mask = batch[1].to(device)\n","          b_labels = batch[2].to(device)\n","\n","          with torch.no_grad():\n","\n","              outputs = model(b_input_ids,\n","                                      attention_mask=b_input_mask,\n","                                      labels=b_labels)\n","\n","          # Accumulate the validation loss.\n","          loss = outputs.loss\n","          logits = outputs.logits\n","          total_eval_loss += loss.item()\n","\n","          # Move logits and labels to CPU\n","          logits = logits.detach().cpu().numpy()\n","          label_ids = b_labels.to('cpu').numpy()\n","\n","          # Calculate the accuracy for this batch of test sentences, and\n","          # accumulate it over all batches.\n","          total_eval_accuracy += flat_accuracy(logits, label_ids)\n","\n","\n","      # Report the final accuracy for this validation run.\n","      avg_train_accuracy = total_train_accuracy / len(train_loader)\n","      avg_val_accuracy = total_eval_accuracy / len(val_loader)\n","      print(f'LR{lr} BS{bs}')\n","      print(\"  Accuracy Val: {0:.2f}\".format(avg_val_accuracy))\n","      print(\"  Accuracy Train: {0:.2f}\".format(avg_train_accuracy))\n","\n","      # Calculate the average loss over all of the batches.\n","      avg_val_loss = total_eval_loss / len(val_loader)\n","\n","      # Measure how long the validation run took.\n","      validation_time = format_time(time.time() - t0)\n","\n","      print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","      print(\"  Validation took: {:}\".format(validation_time))\n","\n","    print(\"\")\n","    print(\"Training complete!\")\n","    model_name = f'/content/gdrive/MyDrive/final_model_lr{lr}_bs{bs}_undersamp{k}.pt'\n","    print(f'model saved {model_name}')\n","    torch.save(model.state_dict(), model_name)\n","\n","    print(\"Total training took {:}\".format(format_time(time.time()-total_t0)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J47ohmQgM501","outputId":"385e9489-bc60-441e-e42e-dcea2885b0fc","executionInfo":{"status":"ok","timestamp":1701642031722,"user_tz":300,"elapsed":1095979,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["undersampled percentage 0.0\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  0%|          | 0/3 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\n"," Epoch 0\n","tensor(1.4105, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9590, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5025, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3114, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0615, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2325, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(1.0399, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4252, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2664, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3430, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5261, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1826, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1353, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2108, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1063, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0813, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2453, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0738, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1282, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0368, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2590, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0306, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2362, device='cuda:0', grad_fn=<NllLossBackward0>)\n","\n","  Average training loss: 0.29\n","\n","Running Validation...\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 1/3 [01:23<02:46, 83.46s/it]"]},{"output_type":"stream","name":"stdout","text":["LR5e-05 BS32\n","  Accuracy Val: 0.00\n","  Accuracy Train: 0.91\n","  Validation Loss: 5.77\n","  Validation took: 0:00:02\n","\n"," Epoch 1\n","tensor(0.1870, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2041, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2073, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0350, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1894, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0320, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0106, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1563, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4483, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1951, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1430, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2278, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1146, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1047, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0770, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3325, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0376, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1374, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0252, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1231, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1027, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1871, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1744, device='cuda:0', grad_fn=<NllLossBackward0>)\n","\n","  Average training loss: 0.11\n","\n","Running Validation...\n"]},{"output_type":"stream","name":"stderr","text":["\r 67%|██████▋   | 2/3 [02:46<01:23, 83.44s/it]"]},{"output_type":"stream","name":"stdout","text":["LR5e-05 BS32\n","  Accuracy Val: 0.00\n","  Accuracy Train: 0.97\n","  Validation Loss: 6.59\n","  Validation took: 0:00:02\n","\n"," Epoch 2\n","tensor(0.0387, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0989, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0082, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0103, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0850, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1370, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0434, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0030, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1700, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4248, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1691, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0057, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0079, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2072, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0393, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1057, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0109, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0070, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0277, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0063, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0056, device='cuda:0', grad_fn=<NllLossBackward0>)\n","\n","  Average training loss: 0.07\n","\n","Running Validation...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [04:10<00:00, 83.44s/it]"]},{"output_type":"stream","name":"stdout","text":["LR5e-05 BS32\n","  Accuracy Val: 0.00\n","  Accuracy Train: 0.98\n","  Validation Loss: 8.66\n","  Validation took: 0:00:02\n","\n","Training complete!\n","model saved /content/gdrive/MyDrive/final_model_lr5e-05_bs32_undersamp0.0.pt\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Total training took 0:04:11\n","undersampled percentage 0.01\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  0%|          | 0/3 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\n"," Epoch 0\n","tensor(1.4556, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.7775, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3444, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1668, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0668, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1320, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1658, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2103, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0906, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2923, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3119, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0492, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1953, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0215, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1460, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1220, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1031, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5125, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0370, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1469, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2346, device='cuda:0', grad_fn=<NllLossBackward0>)\n","\n","  Average training loss: 0.27\n","\n","Running Validation...\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 1/3 [01:23<02:47, 83.70s/it]"]},{"output_type":"stream","name":"stdout","text":["LR5e-05 BS32\n","  Accuracy Val: 0.24\n","  Accuracy Train: 0.91\n","  Validation Loss: 1.80\n","  Validation took: 0:00:02\n","\n"," Epoch 1\n","tensor(0.1277, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0451, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0119, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2149, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0725, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0172, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0508, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0323, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1560, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0300, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0124, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0228, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3691, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1452, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2828, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0219, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1357, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1692, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0457, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1004, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0511, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2093, device='cuda:0', grad_fn=<NllLossBackward0>)\n","\n","  Average training loss: 0.11\n","\n","Running Validation...\n"]},{"output_type":"stream","name":"stderr","text":["\r 67%|██████▋   | 2/3 [02:47<01:23, 83.71s/it]"]},{"output_type":"stream","name":"stdout","text":["LR5e-05 BS32\n","  Accuracy Val: 0.41\n","  Accuracy Train: 0.97\n","  Validation Loss: 2.16\n","  Validation took: 0:00:02\n","\n"," Epoch 2\n","tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0797, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0095, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1348, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0809, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0786, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0052, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2205, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0080, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0862, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0120, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0179, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0593, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0133, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0719, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1964, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0032, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2514, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0132, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0061, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0336, device='cuda:0', grad_fn=<NllLossBackward0>)\n","\n","  Average training loss: 0.07\n","\n","Running Validation...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [04:11<00:00, 83.71s/it]"]},{"output_type":"stream","name":"stdout","text":["LR5e-05 BS32\n","  Accuracy Val: 0.51\n","  Accuracy Train: 0.98\n","  Validation Loss: 2.23\n","  Validation took: 0:00:02\n","\n","Training complete!\n","model saved /content/gdrive/MyDrive/final_model_lr5e-05_bs32_undersamp0.01.pt\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Total training took 0:04:12\n","undersampled percentage 0.1\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  0%|          | 0/3 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\n"," Epoch 0\n","tensor(1.3400, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.8565, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5728, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2723, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2482, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2117, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2592, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1566, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4462, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2093, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0762, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4636, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1576, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1060, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2885, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2000, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0595, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2012, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1729, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0792, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0988, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0917, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0289, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1788, device='cuda:0', grad_fn=<NllLossBackward0>)\n","\n","  Average training loss: 0.29\n","\n","Running Validation...\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 1/3 [01:26<02:52, 86.07s/it]"]},{"output_type":"stream","name":"stdout","text":["LR5e-05 BS32\n","  Accuracy Val: 0.82\n","  Accuracy Train: 0.91\n","  Validation Loss: 0.79\n","  Validation took: 0:00:02\n","\n"," Epoch 1\n","tensor(0.0832, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1617, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0454, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3625, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0137, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0766, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3104, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2817, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0428, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0491, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0352, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3166, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2201, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0441, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0902, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1599, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3024, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4374, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3181, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0148, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0442, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0112, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2095, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0084, device='cuda:0', grad_fn=<NllLossBackward0>)\n","\n","  Average training loss: 0.14\n","\n","Running Validation...\n"]},{"output_type":"stream","name":"stderr","text":["\r 67%|██████▋   | 2/3 [02:52<01:26, 86.06s/it]"]},{"output_type":"stream","name":"stdout","text":["LR5e-05 BS32\n","  Accuracy Val: 0.83\n","  Accuracy Train: 0.96\n","  Validation Loss: 0.91\n","  Validation took: 0:00:02\n","\n"," Epoch 2\n","tensor(0.1985, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0174, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0143, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1531, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0053, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0201, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2117, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0301, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1538, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0208, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0244, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0104, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0081, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0049, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0048, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2027, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0311, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2386, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0274, device='cuda:0', grad_fn=<NllLossBackward0>)\n","\n","  Average training loss: 0.09\n","\n","Running Validation...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [04:18<00:00, 86.06s/it]"]},{"output_type":"stream","name":"stdout","text":["LR5e-05 BS32\n","  Accuracy Val: 0.82\n","  Accuracy Train: 0.98\n","  Validation Loss: 0.93\n","  Validation took: 0:00:02\n","\n","Training complete!\n","model saved /content/gdrive/MyDrive/final_model_lr5e-05_bs32_undersamp0.1.pt\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Total training took 0:04:19\n","undersampled percentage 1\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","  0%|          | 0/3 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\n"," Epoch 0\n","tensor(1.4382, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.9277, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5040, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3580, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1809, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3709, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1857, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3042, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2369, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3276, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2133, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1379, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2562, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.6551, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3347, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1040, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3853, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1087, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1981, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0464, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2776, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0363, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.5171, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0558, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2947, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1928, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1083, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3055, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3333, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4008, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3065, device='cuda:0', grad_fn=<NllLossBackward0>)\n","\n","  Average training loss: 0.31\n","\n","Running Validation...\n"]},{"output_type":"stream","name":"stderr","text":["\r 33%|███▎      | 1/3 [01:49<03:39, 109.70s/it]"]},{"output_type":"stream","name":"stdout","text":["LR5e-05 BS32\n","  Accuracy Val: 0.92\n","  Accuracy Train: 0.90\n","  Validation Loss: 0.36\n","  Validation took: 0:00:02\n","\n"," Epoch 1\n","tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3923, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2153, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0723, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.4078, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0330, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1897, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1551, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1005, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2507, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2588, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1080, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2677, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0614, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1620, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1549, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1990, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1584, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0981, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2074, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3155, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0218, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1102, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2207, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1992, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1888, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0837, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2341, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1871, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2045, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0495, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2261, device='cuda:0', grad_fn=<NllLossBackward0>)\n","\n","  Average training loss: 0.15\n","\n","Running Validation...\n"]},{"output_type":"stream","name":"stderr","text":["\r 67%|██████▋   | 2/3 [03:39<01:49, 109.67s/it]"]},{"output_type":"stream","name":"stdout","text":["LR5e-05 BS32\n","  Accuracy Val: 0.96\n","  Accuracy Train: 0.96\n","  Validation Loss: 0.20\n","  Validation took: 0:00:02\n","\n"," Epoch 2\n","tensor(0.0186, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2180, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0090, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0203, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0088, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2604, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0501, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1876, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3180, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0051, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1676, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2193, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0094, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2526, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1859, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0204, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1490, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0405, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0091, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0875, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0140, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0612, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2204, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1232, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.3781, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.1491, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.2860, device='cuda:0', grad_fn=<NllLossBackward0>)\n","tensor(0.0085, device='cuda:0', grad_fn=<NllLossBackward0>)\n","\n","  Average training loss: 0.10\n","\n","Running Validation...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3/3 [05:28<00:00, 109.66s/it]"]},{"output_type":"stream","name":"stdout","text":["LR5e-05 BS32\n","  Accuracy Val: 0.95\n","  Accuracy Train: 0.98\n","  Validation Loss: 0.22\n","  Validation took: 0:00:02\n","\n","Training complete!\n","model saved /content/gdrive/MyDrive/final_model_lr5e-05_bs32_undersamp1.pt\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Total training took 0:05:30\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"nqgeuasbM8mb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vfYNLSZ9o21N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mQG1KwS-o23j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" def init_model(ckpt_path, device):\n","    # model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=n_lab, output_attentions = False, output_hidden_states=False)\n","    model = DistilBertSequenceClassificationModel()\n","    sd = ch.load(ckpt_path, map_location=torch.device('cuda')) # jyh: (11/18) this worked for the baseline DistilBertSequenceClassificationModel.\n","    # checkpoint = torch.load(CKPT_PATH, map_location=torch.device('cpu'))\n","    # print(checkpoint.keys())\n","    # print(\"type chkpt\", type(checkpoint))\n","    # print(\"type chkpt keys\", type(checkpoint.keys()))\n","    model.model.load_state_dict(sd)\n","    return model"],"metadata":{"id":"MlchAW2Co252"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#model_name = f'/content/drive/MyDrive/model_lr{lr}_bs{bs}_undersamp{k}.pt'\n","import pickle\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","file_name = f'gdrive/MyDrive/final_datasets_0_.01_.1_1.pickle'\n","with open(file_name, 'rb') as handle:\n","    ds = pickle.load(handle)\n","ds\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FvVZ2gR3o28Z","executionInfo":{"status":"ok","timestamp":1701639184443,"user_tz":300,"elapsed":1236,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"40f37b22-fd44-4954-8506-780cd8974ffb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["{0.0: [<torch.utils.data.dataset.TensorDataset at 0x7b10070d4bb0>,\n","  <torch.utils.data.dataset.TensorDataset at 0x7b10070d5270>],\n"," 0.01: [<torch.utils.data.dataset.TensorDataset at 0x7b0ec3d80b50>,\n","  <torch.utils.data.dataset.TensorDataset at 0x7b0ec3d80ca0>],\n"," 0.1: [<torch.utils.data.dataset.TensorDataset at 0x7b0ec3d80670>,\n","  <torch.utils.data.dataset.TensorDataset at 0x7b0ec3d816f0>],\n"," 1: [<torch.utils.data.dataset.TensorDataset at 0x7b0ec3d81b40>,\n","  <torch.utils.data.dataset.TensorDataset at 0x7b0ec3d81de0>]}"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":["for samp_p,datasets in ds.items():\n","  print(samp_p)\n","  train_dataset, val_dataset = datasets\n","  CKPT_PATH = f\"/content/gdrive/MyDrive/mit-nlp-project/model_lr5e-05_bs32_undersamp{samp_p}.pt\"#\"/content/model_lr5e-05_bs32.pt\" # jyh: to load model checkpoints from huggingface: https://huggingface.co/gchhablani/bert-base-cased-finetuned-qnli/tree/main/checkpoint-13094\n"],"metadata":{"id":"kF3sOC9Ao3Sw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"rxX0eOPsqxFq"}},{"cell_type":"code","source":["from transformers import DistilBertForSequenceClassification, AdamW\n","import torch\n"],"metadata":{"id":"DKvv_8g7qsu-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ds.items()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Ffsvqd1mrWW","executionInfo":{"status":"ok","timestamp":1701639278068,"user_tz":300,"elapsed":7,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"35a83914-e02f-45ff-be41-fd40b7a6a9c8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict_items([(0.0, [<torch.utils.data.dataset.TensorDataset object at 0x7b10070d4bb0>, <torch.utils.data.dataset.TensorDataset object at 0x7b10070d5270>]), (0.01, [<torch.utils.data.dataset.TensorDataset object at 0x7b0ec3d80b50>, <torch.utils.data.dataset.TensorDataset object at 0x7b0ec3d80ca0>]), (0.1, [<torch.utils.data.dataset.TensorDataset object at 0x7b0ec3d80670>, <torch.utils.data.dataset.TensorDataset object at 0x7b0ec3d816f0>]), (1, [<torch.utils.data.dataset.TensorDataset object at 0x7b0ec3d81b40>, <torch.utils.data.dataset.TensorDataset object at 0x7b0ec3d81de0>])])"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["import gc\n","from torch.utils.data import DataLoader\n","import numpy as np\n","#torch.cuda.empty_cache()\n","preds_dct = {}\n","for samp_p,v in ds.items():\n","  val_dataset,_ = v\n","  print(f'undersampled percentage {samp_p}')\n","  gc.collect()\n","\n","  device = 'cuda'\n","  #samp_p = str(0.01)\n","  CKPT_PATH =f'/content/gdrive/MyDrive/final_model_lr{lr}_bs{bs}_undersamp{k}.pt'# f\"/content/gdrive/MyDrive/mit-nlp-project/model_lr5e-05_bs32_undersamp{samp_p}.pt\"#\"/content/model_lr5e-05_bs32.pt\" # jyh: to load model checkpoints from huggingface: https://huggingface.co/gchhablani/bert-base-cased-finetuned-qnli/tree/main/checkpoint-13094\n","\n","  model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=5, output_attentions = False, output_hidden_states=False)\n","  model.load_state_dict(torch.load(CKPT_PATH))\n","  model.to(device)\n","\n","  \"\"\"gc.collect()\n","  #torch.cuda.empty_cache()\n","  model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', num_labels=n_lab+1, output_attentions = False, output_hidden_states=False)\n","  model.to(device)\"\"\"\n","\n","  val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","  val_labels = []\n","  val_preds = []\n","  for batch in val_loader:\n","      #\n","      # `batch` contains three pytorch tensors:\n","      #   [0]: input ids\n","      #   [1]: attention masks\n","      #   [2]: labels\n","      b_input_ids = batch[0].to(device)\n","      b_input_mask = batch[1].to(device)\n","      b_labels = batch[2].to(device)\n","\n","      with torch.no_grad():\n","\n","          outputs = model(b_input_ids,\n","                                  attention_mask=b_input_mask,\n","                                  labels=b_labels)\n","\n","      # Accumulate the validation loss.\n","      logits = outputs.logits\n","      val_labels.append(b_labels.cpu().numpy())\n","      val_preds.append(logits.cpu().numpy())\n","\n","  val_labels = np.concatenate(val_labels, axis=0)\n","  val_preds = np.concatenate(val_preds, axis=0)\n","  preds_dct[samp_p] = (val_preds,val_labels)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K0lp7zCyqcQB","executionInfo":{"status":"ok","timestamp":1701639451275,"user_tz":300,"elapsed":118717,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"75bc3c8d-3bb0-4a4d-96ab-2182a8124d06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["undersampled percentage 0.0\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["undersampled percentage 0.01\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["undersampled percentage 0.1\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["undersampled percentage 1\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["import pickle\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","file_name = f'/content/gdrive/MyDrive/mit-nlp-project/final_train_pred_lab_dct.pickle'\n","with open(file_name, 'wb') as handle:\n","    pickle.dump(preds_dct, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wGL__sgptNVc","executionInfo":{"status":"ok","timestamp":1701639832587,"user_tz":300,"elapsed":969,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"cbf9ad11-6cc8-4db2-d155-903227e29cf0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["preds_dct"],"metadata":{"id":"COPnwgaKuYBa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701639817758,"user_tz":300,"elapsed":8,"user":{"displayName":"Danielle Sullivan","userId":"01569278387673466309"}},"outputId":"a55a8b4c-bf12-4659-8204-b10df4b881c2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0.0: (array([[ 0.36643496,  4.865219  , -3.577828  ,  0.11608647, -4.4655585 ],\n","         [-0.93108594,  3.8430438 , -2.1762867 ,  0.3575875 , -3.563869  ],\n","         [-0.40644518,  5.667038  , -3.3975425 ,  0.06959583, -4.524239  ],\n","         ...,\n","         [ 0.772366  ,  4.457282  , -3.6363108 ,  0.42710018, -4.509283  ],\n","         [ 0.20638022,  5.1278844 , -3.5572302 ,  0.1517521 , -4.7263002 ],\n","         [-1.0376931 ,  5.7029366 , -3.086007  ,  0.23465061, -4.2864056 ]],\n","        dtype=float32),\n","  array([1, 1, 1, ..., 1, 1, 1])),\n"," 0.01: (array([[ 0.36643496,  4.865219  , -3.577828  ,  0.11608647, -4.4655585 ],\n","         [-0.93108594,  3.8430438 , -2.1762867 ,  0.3575875 , -3.563869  ],\n","         [-0.40644518,  5.667038  , -3.3975425 ,  0.06959583, -4.524239  ],\n","         ...,\n","         [ 0.772366  ,  4.457282  , -3.6363108 ,  0.42710018, -4.509283  ],\n","         [ 0.20638022,  5.1278844 , -3.5572302 ,  0.1517521 , -4.7263002 ],\n","         [-1.0376931 ,  5.7029366 , -3.086007  ,  0.23465061, -4.2864056 ]],\n","        dtype=float32),\n","  array([1, 1, 1, ..., 1, 1, 1])),\n"," 0.1: (array([[ 0.36643496,  4.865219  , -3.577828  ,  0.11608647, -4.4655585 ],\n","         [-0.93108594,  3.8430438 , -2.1762867 ,  0.3575875 , -3.563869  ],\n","         [-0.40644518,  5.667038  , -3.3975425 ,  0.06959583, -4.524239  ],\n","         ...,\n","         [ 0.772366  ,  4.457282  , -3.6363108 ,  0.42710018, -4.509283  ],\n","         [ 0.20638022,  5.1278844 , -3.5572302 ,  0.1517521 , -4.7263002 ],\n","         [-1.0376931 ,  5.7029366 , -3.086007  ,  0.23465061, -4.2864056 ]],\n","        dtype=float32),\n","  array([1, 1, 1, ..., 1, 1, 1])),\n"," 1: (array([[-1.5415776 , -1.3819084 , -2.169578  ,  6.3311534 , -3.8299508 ],\n","         [ 0.36643496,  4.865219  , -3.577828  ,  0.11608647, -4.4655585 ],\n","         [-0.93108594,  3.8430438 , -2.1762867 ,  0.3575875 , -3.563869  ],\n","         ...,\n","         [ 0.772366  ,  4.457282  , -3.6363108 ,  0.42710018, -4.509283  ],\n","         [ 0.20638022,  5.1278844 , -3.5572302 ,  0.1517521 , -4.7263002 ],\n","         [-1.0376931 ,  5.7029366 , -3.086007  ,  0.23465061, -4.2864056 ]],\n","        dtype=float32),\n","  array([3, 1, 1, ..., 1, 1, 1]))}"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":[],"metadata":{"id":"5Zz8vTzfowWv"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1Gd2jpf5M-I6-BJBO8ZcvrakvnzXy0Hzg","authorship_tag":"ABX9TyNU7WVGWrIcy4+/ptfm0qeA"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}